{
 "metadata": {
  "name": "",
  "signature": "sha256:73dc5e5edac6095a944c0c9a629e01827385d0b251f6fc7d8ca9f04a693b9bd2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Cross-Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "from sklearn.cross_validation import train_test_split, cross_val_score\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
      "from sklearn import metrics\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What's Wrong with Train/Test Split for Model Evaluation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we studied the **train/test split** procedure for model evaluation. We found that it was a superior alternative to training and testing on the same data because it helps to avoid overfitting.\n",
      "\n",
      "However, we also found that it provides a **high variance estimate** of out-of-sample accuracy, since changing which observations happen to be in the training set versus the testing set can make a meaningful difference in the estimated accuracy.\n",
      "\n",
      "Let's see this in code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the iris data\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use train/test split with random_state=4\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
      "\n",
      "# check accuracy of KNN with K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(X_train, y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "print metrics.accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use train/test split with random_state=1\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "# check accuracy of KNN with K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(X_train, y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "print metrics.accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One **solution** to this problem might be to take a bunch of train/test splits, and then average the results together. That's the essense of cross-validation!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Steps for K-fold Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Randomly split the dataset into K equal partitions.\n",
      "2. Use partition 1 as the test set and the union of the other partitions as the training set.\n",
      "3. Calculate test set accuracy.\n",
      "4. Repeat steps 2 and 3 K times, using a different partition as the test set each time.\n",
      "5. Use the average test set accuracy as the estimate of out-of-sample accuracy.\n",
      "\n",
      "Here are two diagrams of **5-fold cross-validation:**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![5-fold cross-validation](images/cross_validation_diagram.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![5-fold cross-validation](images/cross_validation_example.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross-Validation Recommendations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Any number of folds can be used, but **10-fold cross-validation** is generally recommended because it has been shown experimentally to produce the most reliable estimates of out-of-sample accuracy.\n",
      "\n",
      "For classification problems, **stratified sampling** is also recommended, meaning that each response class should be represented with approximately equal proportions in each of the K folds. (scikit-learn does by default when using `cross_val_score`, which we will use below.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparing Cross-Validation to Train/Test Split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Advantages of **cross-validation:**\n",
      "\n",
      "- More accurate estimate of out-of-sample accuracy\n",
      "- More efficient use of data (every record is used for both training and testing)\n",
      "\n",
      "Advantages of **train/test split:**\n",
      "\n",
      "- Runs 10 times faster than 10-fold cross-validation\n",
      "- Simpler to examine the test set results without writing custom code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross-Validation Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try out cross-validation on a few datasets:\n",
      "\n",
      "- **iris:** multi-class classification\n",
      "- **default:** binary classification\n",
      "- **advertising:** regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Iris dataset\n",
      "\n",
      "- Multi-class classification\n",
      "- Using accuracy as evaluation metric\n",
      "- Using cross-validation to select tuning parameters (aka \"hyperparameters\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the iris data\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10-fold cross-validation with K=5\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use average accuracy as estimate of out-of-sample accuracy\n",
      "print np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# search for an optimal value of K\n",
      "k_range = range(1, 30)\n",
      "scores = []\n",
      "for k in k_range:\n",
      "    knn = KNeighborsClassifier(n_neighbors=k)\n",
      "    scores.append(np.mean(cross_val_score(knn, X, y, cv=10, scoring='accuracy')))\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the K values (x-axis) versus the 10-fold CV score (y-axis)\n",
      "plt.plot(k_range, scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Default dataset\n",
      "\n",
      "- Binary classification\n",
      "- Using AUC as evaluation metric\n",
      "- Using cross-validation to select between models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the default data\n",
      "data = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/default.csv')\n",
      "X = data[['balance']]\n",
      "y = data.default"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10-fold cross-validation with logistic regression\n",
      "logreg = LogisticRegression()\n",
      "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10-fold cross-validation with KNN (K=5)\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "cross_val_score(knn, X, y, cv=10, scoring='roc_auc').mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Advertising dataset\n",
      "\n",
      "- Regression\n",
      "- Using RMSE as evaluation metric\n",
      "- Using cross-validation to select features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the advertising data\n",
      "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10-fold cross-validation with three features\n",
      "X = data[['TV', 'Radio', 'Newspaper']]\n",
      "y = data.Sales\n",
      "lm = LinearRegression()\n",
      "scores = cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert from MSE to RMSE\n",
      "scores_sqrt = np.sqrt(-scores)\n",
      "print scores_sqrt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the average RMSE\n",
      "print np.mean(scores_sqrt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 10-fold cross-validation with two features\n",
      "feature_cols = ['TV', 'Radio']\n",
      "X = data[feature_cols]\n",
      "print np.mean(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Improvements to Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although standard K-fold cross-validation is a very useful model evaluation procedure, there are some common variations that can make cross-validation even better:\n",
      "\n",
      "### Repeated cross-validation\n",
      "\n",
      "K-fold cross-validation is repeated multiple times (with different random splits of the data into the K folds), and the results are averaged. This provides a more reliable estimate of out-of-sample accuracy by reducing the variance associated with a single trial of cross-validation.\n",
      "\n",
      "### Creating a hold-out set\n",
      "\n",
      "Instead of running cross-validation on the entire dataset, a portion of the data is \"held out\" and not touched during the model building process. The best model is located and tuned using cross-validation on the remaining data. At the end of this process, the hold-out set is then used to test the best model. The hold-out set accuracy is considered to be a more reliable estimate of out-of-sample accuracy than the cross-validated accuracy, since the hold-out set is truly out-of-sample data.\n",
      "\n",
      "### Feature engineering and selection within cross-validation iterations\n",
      "\n",
      "Ideally, all feature engineering and selection should be done within each cross-validation iteration. Performing these tasks before cross-validation does not properly mimic the application of the model to out-of-sample data, since those processes will have \"unfair\" knowledge of the entire dataset, and thus the cross-validated estimate of out-of-sample accuracy will be biased upward. Performing these tasks within cross-validation iterations will produce a more reliable estimate."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}