{
 "metadata": {
  "name": "",
  "signature": "sha256:73c3d3b112e665a0ca055b18b6bdd557da4c96c0f2aef4334841df43fcf1df81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Natural Language Processing (NLP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is NLP?\n",
      "* 'Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human\u2013computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input...' - Wikipedia\n",
      "* Using computers to process (analyze, understand, generate) natural human languages (as opposed to unnatural computer languages).\n",
      "* NLP is concerned with the interface between human and computer language.  However, language is often ambiguous, so this isn't always a straightforward task.\n",
      "\n",
      "Why NLP?\n",
      "* Most knowledge created by humans is unstructured text, so we need some way to make sense of it.\n",
      "* Enables quantitative analysis of text data at large scale.\n",
      "* Provides a repeatable, \"unbiased\" way to look at text."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Motivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the purpose of this class, we can pretend we are Data Scientists working for Kaggle.  Everyone loves Kaggle, but we're interested into digging a little deeper into what the web is saying about Kaggle."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Imports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy # Twitter API wrapper\n",
      "import nltk # Classic NLP package"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Variables that contains the user credentials to access Twitter API \n",
      "consumer_key = \"Consumer_Key\" # Replace with your own consumer_key\n",
      "consumer_secret = \"Consumer_Secret\" # Replace with your own consumer_secret\n",
      "\n",
      "# Create authorization for API\n",
      "auth = tweepy.auth.OAuthHandler(consumer_key, consumer_secret)\n",
      "#auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "# Initialize API object by passing it your credentials\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "# Use the api to search\n",
      "tweets = api.search(q=\"kaggle\", count=10, result_type=\"recent\")\n",
      "print tweets[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Status(contributors=None, truncated=False, text=u'37 spots up. Is anybody else even trying? #kaggle - https://t.co/9GLyaK3OhT', in_reply_to_status_id=None, id=595283912592588800L, favorite_count=0, _api=<tweepy.api.API object at 0x000000002631D080>, author=User(follow_request_sent=None, profile_use_background_image=True, _json={u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 88491546, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'description': {u'urls': []}}, u'followers_count': 14, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'88491546', u'profile_background_color': u'C0DEED', u'listed_count': 0, u'is_translation_enabled': False, u'utc_offset': -7200, u'statuses_count': 10, u'description': u'', u'friends_count': 66, u'location': u'', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'following': None, u'geo_enabled': True, u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'KidOfSahel', u'lang': u'fr', u'profile_background_tile': False, u'favourites_count': 4, u'name': u'Mamadou Diaby', u'notifications': None, u'url': None, u'created_at': u'Sun Nov 08 19:39:55 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Greenland', u'protected': False, u'default_profile': True, u'is_translator': False}, time_zone=u'Greenland', id=88491546, _api=<tweepy.api.API object at 0x000000002631D080>, verified=False, profile_text_color=u'333333', profile_image_url_https=u'https://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', profile_sidebar_fill_color=u'DDEEF6', is_translator=False, geo_enabled=True, entities={u'description': {u'urls': []}}, followers_count=14, protected=False, id_str=u'88491546', default_profile_image=False, listed_count=0, lang=u'fr', utc_offset=-7200, statuses_count=10, description=u'', friends_count=66, profile_link_color=u'0084B4', profile_image_url=u'http://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', notifications=None, profile_background_image_url_https=u'https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_color=u'C0DEED', profile_background_image_url=u'http://abs.twimg.com/images/themes/theme1/bg.png', name=u'Mamadou Diaby', is_translation_enabled=False, profile_background_tile=False, favourites_count=4, screen_name=u'KidOfSahel', url=None, created_at=datetime.datetime(2009, 11, 8, 19, 39, 55), contributors_enabled=False, location=u'', profile_sidebar_border_color=u'C0DEED', default_profile=True, following=False), _json={u'contributors': None, u'truncated': False, u'text': u'37 spots up. Is anybody else even trying? #kaggle - https://t.co/9GLyaK3OhT', u'in_reply_to_status_id': None, u'id': 595283912592588800L, u'favorite_count': 0, u'source': u'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>', u'retweeted': False, u'coordinates': None, u'entities': {u'symbols': [], u'user_mentions': [], u'hashtags': [{u'indices': [42, 49], u'text': u'kaggle'}], u'urls': [{u'url': u'https://t.co/9GLyaK3OhT', u'indices': [52, 75], u'expanded_url': u'https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot', u'display_url': u'kaggle.com/c/facebook-rec\\u2026'}]}, u'in_reply_to_screen_name': None, u'in_reply_to_user_id': None, u'retweet_count': 0, u'id_str': u'595283912592588800', u'favorited': False, u'user': {u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 88491546, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'description': {u'urls': []}}, u'followers_count': 14, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'88491546', u'profile_background_color': u'C0DEED', u'listed_count': 0, u'is_translation_enabled': False, u'utc_offset': -7200, u'statuses_count': 10, u'description': u'', u'friends_count': 66, u'location': u'', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'following': None, u'geo_enabled': True, u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'KidOfSahel', u'lang': u'fr', u'profile_background_tile': False, u'favourites_count': 4, u'name': u'Mamadou Diaby', u'notifications': None, u'url': None, u'created_at': u'Sun Nov 08 19:39:55 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Greenland', u'protected': False, u'default_profile': True, u'is_translator': False}, u'geo': None, u'in_reply_to_user_id_str': None, u'possibly_sensitive': False, u'lang': u'en', u'created_at': u'Mon May 04 17:48:39 +0000 2015', u'in_reply_to_status_id_str': None, u'place': None, u'metadata': {u'iso_language_code': u'en', u'result_type': u'recent'}}, coordinates=None, entities={u'symbols': [], u'user_mentions': [], u'hashtags': [{u'indices': [42, 49], u'text': u'kaggle'}], u'urls': [{u'url': u'https://t.co/9GLyaK3OhT', u'indices': [52, 75], u'expanded_url': u'https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot', u'display_url': u'kaggle.com/c/facebook-rec\\u2026'}]}, in_reply_to_screen_name=None, in_reply_to_user_id=None, retweet_count=0, id_str=u'595283912592588800', favorited=False, source_url=u'http://twitter.com', user=User(follow_request_sent=None, profile_use_background_image=True, _json={u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 88491546, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'description': {u'urls': []}}, u'followers_count': 14, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'88491546', u'profile_background_color': u'C0DEED', u'listed_count': 0, u'is_translation_enabled': False, u'utc_offset': -7200, u'statuses_count': 10, u'description': u'', u'friends_count': 66, u'location': u'', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', u'following': None, u'geo_enabled': True, u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'KidOfSahel', u'lang': u'fr', u'profile_background_tile': False, u'favourites_count': 4, u'name': u'Mamadou Diaby', u'notifications': None, u'url': None, u'created_at': u'Sun Nov 08 19:39:55 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Greenland', u'protected': False, u'default_profile': True, u'is_translator': False}, time_zone=u'Greenland', id=88491546, _api=<tweepy.api.API object at 0x000000002631D080>, verified=False, profile_text_color=u'333333', profile_image_url_https=u'https://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', profile_sidebar_fill_color=u'DDEEF6', is_translator=False, geo_enabled=True, entities={u'description': {u'urls': []}}, followers_count=14, protected=False, id_str=u'88491546', default_profile_image=False, listed_count=0, lang=u'fr', utc_offset=-7200, statuses_count=10, description=u'', friends_count=66, profile_link_color=u'0084B4', profile_image_url=u'http://pbs.twimg.com/profile_images/2112597225/421667_3138983843457_1530736888_3009828_102270204_n_normal.jpg', notifications=None, profile_background_image_url_https=u'https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_color=u'C0DEED', profile_background_image_url=u'http://abs.twimg.com/images/themes/theme1/bg.png', name=u'Mamadou Diaby', is_translation_enabled=False, profile_background_tile=False, favourites_count=4, screen_name=u'KidOfSahel', url=None, created_at=datetime.datetime(2009, 11, 8, 19, 39, 55), contributors_enabled=False, location=u'', profile_sidebar_border_color=u'C0DEED', default_profile=True, following=False), geo=None, in_reply_to_user_id_str=None, possibly_sensitive=False, lang=u'en', created_at=datetime.datetime(2015, 5, 4, 17, 48, 39), in_reply_to_status_id_str=None, place=None, source=u'Twitter Web Client', retweeted=False, metadata={u'iso_language_code': u'en', u'result_type': u'recent'})\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets[0].text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "u'37 spots up. Is anybody else even trying? #kaggle - https://t.co/9GLyaK3OhT'"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some quick vocab:\n",
      "* **corpus** - collection of documents\n",
      "* **corpora** - plural form of corpus\n",
      "\n",
      "Let's build our corpus of tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets_text = []\n",
      "for tweet in tweepy.Cursor(api.search, q='kaggle', result_type='recent').items(1000):\n",
      "    tweets_text.append(tweet.text.encode('ascii','ignore'))\n",
      "print tweets_text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37 spots up. Is anybody else even trying? #kaggle - https://t.co/9GLyaK3OhT\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since I did not provide my credentials above, I saved all of the tweet text to a CSV file.  We can read it back into a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Running this will overwrite the current data.\n",
      "'''\n",
      "with open('../data/kaggle_tweets.csv','w') as f:\n",
      "    for tweet in tweets_text:\n",
      "        f.write('\"%s\"\\n' % tweet)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('../data/kaggle_tweets.csv','r') as f:\n",
      "    tweets_text = [tweet.replace('\\n','').replace('\"','') for tweet in f.readlines()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets_text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we need to do with our corpus of text is to break the documents into smaller units.  This is known as **tokenization**.  There are two natural ways to go about this:  breaking the documents apart into sentences or into words.  This gives more structure to the previously unstructured text.  This also allows us to more easily perform other tasks upon our corpus.\n",
      "\n",
      "**Note**:  Breaking documents and paragraphs into sentences and words is easier is some languages than others.  English has obvious (to us) breaks in the text for sentences and words.  However, this might not be the case for other languages.  In addition, there are nuances in the English language with hyphenated words and phrases that are independent clauses but might be part of a larger sentence.\n",
      "\n",
      "First let's try breaking our tweets down into sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize into sentences\n",
      "sentences = []\n",
      "for tweet in tweets_text:\n",
      "    for sent in nltk.sent_tokenize(tweet):\n",
      "        sentences.append(sent)\n",
      "sentences[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's break our tweets into individual words, referred to as tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize into words\n",
      "tokens = []\n",
      "for tweet in tweets_text:\n",
      "    for word in nltk.word_tokenize(tweet):\n",
      "        tokens.append(word)\n",
      "tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is really messy though.  Do we care about analyzing punctuation and other non alphanumeric characters?  We will exclude those using regular expressions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Only keep tokens that start with a letter (using regular expressions)\n",
      "import re\n",
      "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
      "clean_tokens[:20]# Tokenize into words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now perform the \"hello world\" task of text analysis and get a list of the most popular words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count the tokens\n",
      "from collections import Counter\n",
      "c = Counter(clean_tokens)\n",
      "c.most_common(25) # Most frequent tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What do you notice about this list of words?  Are there any duplicated words?  What should we do about that?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming and Lemmatizing (Normalizing)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stemming** reduces a word to its base (stem) form.  It often makes sense to treat multipe word forms the same way.  \n",
      "\n",
      "Stemming uses a \"simple\" rule-based approach that runs very quickly.  The output isn't always the best for irregular words.  Stemmed words are not usually shown to users but rather used for analysis/indexing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize stemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "stemmer = SnowballStemmer('english')\n",
      "\n",
      "# Some exmaples\n",
      "print 'charge:', stemmer.stem('charge')\n",
      "print 'charging:', stemmer.stem('charging')\n",
      "print 'charged:', stemmer.stem('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's stem all of our tokens and recompute the count of most popular tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stem the tokens\n",
      "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
      "\n",
      "# Count the stemmed tokens\n",
      "c = Counter(stemmed_tokens)\n",
      "c.most_common(25)       # all lowercase"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, some of these are still a bit uninterpretable for humans.  That's where lemmatizing comes in.\n",
      "\n",
      "**Lemmatization**, or normalization, dervies the canonical form (i.e. lemma) of a word.  This can be better than stemming in some cases, because it reduces words to a \"normal\" form.  This often uses a dictionary based approach and can be slower than stemming.  This is the tradeoff for \"better\" results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize lemmatizer\n",
      "lemmatizer = nltk.WordNetLemmatizer()\n",
      "\n",
      "# Compare stemmer to lemmatizer\n",
      "print 'dogs - stemmed:', stemmer.stem('dogs'), ', lemmatized:', lemmatizer.lemmatize('dogs')\n",
      "\n",
      "print 'wolves - stemmed:', stemmer.stem('wolves'), ', lemmatized:', lemmatizer.lemmatize('wolves')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's lemmatize our Twitter dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lemmatize the tokens\n",
      "lemmatized_tokens = [lemmatizer.lemmatize(t).lower() for t in clean_tokens] # I lowercased things too.\n",
      "\n",
      "# Count the lemmatized tokens\n",
      "c = Counter(lemmatized_tokens)\n",
      "c.most_common(25)       # all lowercase"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lemmatizing didn't do much here since msot of the popular words don't have significantly different normal forms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# One more example\n",
      "print 'is - stemmed:', stemmer.stem('is'), ', lemmatized:', lemmatizer.lemmatize('is')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is not what I learned in grammar school.  Why isn't the result \"be\"? \n",
      "\n",
      "The lemmatizer assumes everything is a noun unless explicitly told otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer.lemmatize('is',pos='v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stopword Removal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stopwords** are common words that will most likely appear in any text.  They are \"useless\" words that don't contain much information.  For the purpose of word counts and other word frequencies, they are not particularly useful.  \n",
      "\n",
      "Let's remove the stopwords from our tweets and look at the most popular words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# View the list of stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "print stopwords[0:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have a list of stopwords, we can remove them from our token list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stem the stopwords\n",
      "stemmed_stops = [stemmer.stem(t) for t in stopwords]\n",
      "\n",
      "# Remove stopwords from stemmed tokens\n",
      "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in stemmed_stops]\n",
      "c = Counter(stemmed_tokens_no_stop)\n",
      "most_common_stemmed = c.most_common(25)\n",
      "\n",
      "# Remove stopwords from cleaned tokens\n",
      "clean_tokens_no_stop = [t.lower() for t in clean_tokens if t.lower() not in stopwords]\n",
      "c = Counter(clean_tokens_no_stop)\n",
      "most_common_not_stemmed = c.most_common(25)\n",
      "\n",
      "# Compare the most common results for stemmed words and non stemmed words\n",
      "for i in range(25):\n",
      "    text_list = most_common_stemmed[i][0] + '  ' + str(most_common_stemmed[i][1]) + ' '*25\n",
      "    text_list = text_list[0:30]\n",
      "    text_list += most_common_not_stemmed[i][0] + '  ' + str(most_common_not_stemmed[i][1])\n",
      "    print text_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These results are a bit more interesting.  You can see the most popular words that occur in the tweets about \"kaggle\".  We could dig further into this and look at the specific tweets for some of the more interesting occurences.  For instance, it seeems that there are more occurences of \"R\" than \"Python\" (which doesn't even show up in our top list).  Does that mean that R is more popular than Python for Kaggle competitions?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Named Entity Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Named Entity Recognition (NER)** is the automatic extraction of names, places, organizations, etc.  This can help you identify \"important\" words.  NER classifiers can work in different ways, but the most interesting and relevant ones use some sort of supervised machine learning technique.  There is some sort of tagged dataset that has a model/algorithm fit to it.  With what we've learned in class so far, you could build your own NER classifier!  However, it's often better to use existing classifiers.  \n",
      "\n",
      "First, let's build a NER extraction function that takes in a sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_entities(text):\n",
      "    entities = []\n",
      "    # tokenize into sentences\n",
      "    for sentence in nltk.sent_tokenize(text):\n",
      "        # tokenize sentences into words\n",
      "        # add part-of-speech tags\n",
      "        # use NLTK's NER classifier\n",
      "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
      "        # parse the results\n",
      "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
      "    return entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at all of the words in this dataset and see which named entities are identified.\n",
      "for entity in extract_entities('Kevin and Brandon are instructors for General Assembly in Washington, D.C.'):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems to work pretty well!  But how resilient is it?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for entity in extract_entities('kevin and BRANDON are instructors for @GA_DC, D.C.'):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The accuracy decreased dramatically!  There are companies who are working to solve this problem, but as you get into more unstructured, \"wild\" data (like social media), this gets harder to do. \n",
      "\n",
      "We could run this on our entire dataset, but it would take a while, so I'll provide the code, but not actually run it.\n",
      "\n",
      "``` {python}\n",
      "named_entities = []\n",
      "for tweet in tweets_text:\n",
      "    temp_entities = extract_entities(tweet)\n",
      "    for temp_entity in temp_entities:\n",
      "        named_entities.append((temp_entity.label(), temp_entity.leaves()[0][0]))\n",
      "```\n",
      "\n",
      "Let's at least run it on one tweet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tweets_text[21]\n",
      "for entity in extract_entities(tweets_text[21]):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Topic Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic modeling allows us to discover \"topic grougs\" in our dataset.  There are several different versions of this, but we'll talk about one specifically, LDA.\n",
      "\n",
      "**Latent Dirichlet Allocation (LDA)** is a topic modeling method that allows us to discover clusters of words that appear together frequently.  We can use this to look for clusters of words in our Kaggle corpus.\n",
      "\n",
      "While the code below intorduces some new specifics, the overall process is similar to what we've done before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lda # Latent Dirichlet Allocation\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Instantiate a count vectorizer with two additional parameters\n",
      "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
      "sentences_train = vect.fit_transform(np.array(tweets_text))\n",
      "\n",
      "# Instantiate an LDA model\n",
      "model = lda.LDA(n_topics=10, n_iter=500)\n",
      "model.fit(sentences_train) # Fit the model \n",
      "n_top_words = 10\n",
      "topic_word = model.topic_word_\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These results could be interesting.  There are vague clusters about machinelearning/scikit learn, methods of classification, and others.  These are not hard and fast clusters or groups, but they are something to investigate.\n",
      "\n",
      "Let's try this again on a different corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Get Data Science Wiki page\n",
      "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
      "b = BeautifulSoup(r.text)\n",
      "paragraphs = b.find(\"body\").findAll(\"p\")\n",
      "paragraphs_text = [p.text for p in paragraphs]\n",
      "text = \"\"\n",
      "for paragraph in paragraphs:\n",
      "    text += paragraph.text + \" \"\n",
      "\n",
      "# Data Science corpus\n",
      "text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into sentences\n",
      "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
      "sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can try running LDA using the paragraphs as documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instantiate a count vectorizer with two additional parameters\n",
      "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
      "sentences_train = vect.fit_transform(paragraphs_text)\n",
      "\n",
      "# Instantiate an LDA model\n",
      "model = lda.LDA(n_topics=10, n_iter=500)\n",
      "model.fit(sentences_train) # Fit the model \n",
      "n_top_words = 10\n",
      "topic_word = model.topic_word_\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use the sentences as documents.  While topic modeling usually does better with longer documents (emails vs. tweets), LDA has been shown to do well even with short documents (specifically tweets)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instantiate a count vectorizer with two additional parameters\n",
      "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
      "sentences_train = vect.fit_transform(paragraphs_text)\n",
      "\n",
      "# Instantiate an LDA model\n",
      "model = lda.LDA(n_topics=10, n_iter=500)\n",
      "model.fit(sentences_train) # Fit the model \n",
      "n_top_words = 10\n",
      "topic_word = model.topic_word_\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since topic modeling is more of a clustering technique, it can be difficult to determine which one is \"better\"."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Textblob"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's talk about a new NLP package, Textblob.  Textblob's tagline is \"Simplified Text Processing\".  You can do many of the same things in Textblob that you can do in NLTK, but it is \"simpler\" to use.  That's obviously a subjective thing, but it's good to be aware of both packages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob, Word\n",
      "\n",
      "# Textblob has a different syntax, but it generally performs the same functions as NLTK.\n",
      "blob = TextBlob('Kevin and Brandon are instructors for General Assembly in Washington, D.C.  They both love Data Science.')\n",
      "print 'Sentences:', blob.sentences\n",
      "print 'Words:', blob.words\n",
      "print 'Noun Phrases:', blob.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Textblob has many useful functionalities:  \n",
      "* Singularizing and pluralizing words\n",
      "* Spell check\n",
      "* Word defintions\n",
      "* Translation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Singularize and pluralize\n",
      "blob = TextBlob('Put away the dishes.')\n",
      "print [word.singularize() for word in blob.words]\n",
      "print [word.pluralize() for word in blob.words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Spelling correction\n",
      "blob = TextBlob('15 minuets late')\n",
      "print 'Original: 15 minuets late    Corrected:', blob.correct()\n",
      "\n",
      "# Spellcheck\n",
      "print 'Original: parot    Corrected:', Word('parot').spellcheck()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Definitions\n",
      "print Word('bank').define()\n",
      "print ' '\n",
      "print Word('bank').define('v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# translation and language identification\n",
      "blob = TextBlob('Welcome to the classroom.')\n",
      "print 'English: \"Welcome to the classroom.\"    Spanish:', blob.translate(to='es')\n",
      "print ''\n",
      "blob = TextBlob('Hola amigos')\n",
      "print '\"Hola amigos\" is the language', blob.detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentiment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentiment allows us to convert a limited range of emotion into a number.  It gives us an idea of how \"positive\", \"negative\", or \"neutral\" a piece of text is.  We built a sentiment API function in our API's class, so we could use that.  But for the sake of variety, let's use the built in functionality of Textblob.\n",
      "\n",
      "Textblob has two different \"types\" of sentiment, a polarity sentiment (positive/negative) and a subjectivity sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The sentiment polarity score is a float within the range [-1.0, 1.0].\n",
      "print 'I love pizza    Sentiment =', TextBlob('I love pizza').sentiment.polarity\n",
      "print 'I hatee pizza    Sentiment =', TextBlob('I hate pizza').sentiment.polarity\n",
      "print 'I feel nothing about pizza    Sentiment =', TextBlob('I feel nothing about pizza').sentiment.polarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
      "print 'I am a cool person    Subjectivity =', TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
      "print 'I am a person    Subjectivity =', TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But once again, it's not perfect."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# different scores for essentially the same sentence\n",
      "print TextBlob('Kevin and Brandon are instructors for General Assembly in Washington, D.C.').sentiment.subjectivity\n",
      "print TextBlob('Kevin and Brandon are instructors in Washington, D.C.').sentiment.subjectivity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this idea of sentiment in mind, let's see how positive, negative, and neutral people are about our Kaggle tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's loop through our tweets and calculate sentiment\n",
      "sentiments = [TextBlob(tweet).sentiment.polarity for tweet in tweets_text]\n",
      "print tweets_text[0], sentiments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Average sentiment\n",
      "avg_sentiment = np.sum(sentiments)/len(sentiments)\n",
      "print avg_sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This average sentiment is pretty neutral.  In my experience, this is usually the case; people don't express as much sentiment as you think and the positives and negatives often cancel each other out.\n",
      "\n",
      "Let's look at the distribution of the sentiment to get a better idea."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.distplot(sentiments)\n",
      "plt.title('Distribution of Sentiment')\n",
      "plt.xlabel('Sentiment (-1 to 1)')\n",
      "plt.ylabel('Frequency')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like people tend to be more positive about Kaggle than negative.\n",
      "\n",
      "Let's look at the really negative tweets!  We'll define negative as less than or equal to -0.25.  We'll exclude all the tweets with links in them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop through sentiments and look for negative sentiments. \n",
      "for i in range(len(sentiments)):\n",
      "    if sentiments[i] <= -0.25 and 'http' not in tweets_text[i]:\n",
      "        print tweets_text[i], sentiments[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the postive ones, too.  We'll define positive as greater than or equal to 0.25."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop through sentiments and look for positive sentiments. \n",
      "for i in range(len(sentiments)):\n",
      "    if sentiments[i] >= 0.25 and 'http' not in tweets_text[i]:\n",
      "        print tweets_text[i], sentiments[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems like there might be some false positives in there.  However, it's what we have to work with, so let's get a count of the \"negative\" (-1.00 to -0.25), \"neutral\" (-0.25 to 0.25), and \"positive\" (0.25 to 1.00) tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop through all of the sentiments and put them into the appropriate group\n",
      "pos_neg_neutral = []\n",
      "for sentiment in sentiments:\n",
      "    if sentiment <= -0.25:\n",
      "        pos_neg_neutral.append('negative')\n",
      "    elif sentiment >= 0.25:\n",
      "        pos_neg_neutral.append('positive')\n",
      "    elif sentiment > -0.25 and sentiment < 0.25:\n",
      "        pos_neg_neutral.append('neutral')\n",
      "\n",
      "sns.barplot(np.array(pos_neg_neutral))\n",
      "plt.title('Positive, Negative, and Neutral Sentiment')\n",
      "plt.xlabel('Sentiment Category')\n",
      "plt.ylabel('Frequency')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that most tweets are neutral, but there are far more positive tweets than negative tweets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've made a few interesting discoveries here, but there is a lot more that could be explored.  You could collect more data, look at the sentiment for specific topics, track topic velocity over time, explore more complex topic modeling, and much more.  Natural Language Processing is a vast field with many subareas.  We've provided some links on the DAT5 readme to help you explore it more deeply."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}