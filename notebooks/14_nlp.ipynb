{
 "metadata": {
  "name": "",
  "signature": "sha256:71c953bde7e16f096b2b6fbcb6fb62e92c2a1abdd8c199847d6d46f5e20bfb03"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Natural Language Processing (NLP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is NLP?\n",
      "* Using computers to process (analyze, understand, generate) natural human languages\n",
      "\n",
      "Why NLP?\n",
      "* Most knowledge created by humans is unstructured text, so we need some way to make sense of it.\n",
      "* Enables quantitative analysis of text data at large scale.\n",
      "* Provides a repeatable, \"unbiased\" way to look at text."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Imports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests # Python going to the Internet\n",
      "from bs4 import BeautifulSoup # Create structured object from HTML\n",
      "import nltk # Classic NLP package\n",
      "import textblob # NLP package\n",
      "import lda # Latent Dirichlet Allocation\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some quick vocab:\n",
      "* \"corpus\" = collection of documents\n",
      "* \"corpora\" = plural form of corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We need a corpus to work with\n",
      "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\") # Go to Wikipedia\n",
      "b = BeautifulSoup(r.text) # Turn HTML into structured object\n",
      "paragraphs = b.find(\"body\").findAll(\"p\") # Find all paragraghs in the article\n",
      "\n",
      "# Turn the paragraphs into one long text string\n",
      "text = \"\"\n",
      "for paragraph in paragraphs:\n",
      "    text += paragraph.text + \" \"\n",
      "    \n",
      "# Data Science corpus\n",
      "text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "u'In general terms, Data Science is the extraction of knowledge from data.[1][2] It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information theory and information technology, including signal processing, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression and high'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we need to do with our corpus of text is to break the documents into smaller units.  This is known as \"tokenization\".  There are two natural ways to go about this:  breaking the documents apart into sentences or into words.  This gives more structure to the previously unstructured text.  This also allows us to more easily perform other tasks upon our corpus.\n",
      "\n",
      "**Note**:  Breaking documents and paragraphs into sentences and words is easier is some languages than others.  English has obvious (to us) breaks in the text for sentences and words.  However, this might not be the case for other languages.  In addition, there are nuances in the English language with hyphenated words and phrases that are independent clauses but might be part of a larger sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize into sentences\n",
      "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
      "sentences[:10]\n",
      "\n",
      "# Tokenize into words\n",
      "tokens = [word for word in nltk.word_tokenize(text)]\n",
      "tokens[:100]\n",
      "\n",
      "# Only keep tokens that start with a letter (using regular expressions)\n",
      "import re\n",
      "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
      "clean_tokens[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[u'In',\n",
        " u'general',\n",
        " u'terms',\n",
        " u'Data',\n",
        " u'Science',\n",
        " u'is',\n",
        " u'the',\n",
        " u'extraction',\n",
        " u'of',\n",
        " u'knowledge',\n",
        " u'from',\n",
        " u'data',\n",
        " u'It',\n",
        " u'employs',\n",
        " u'techniques',\n",
        " u'and',\n",
        " u'theories',\n",
        " u'drawn',\n",
        " u'from',\n",
        " u'many']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now perform the \"hello world\" task of text analysis and get a list of the most popular words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count the tokens\n",
      "from collections import Counter\n",
      "c = Counter(clean_tokens)\n",
      "c.most_common(25)       # Mixed case\n",
      "sorted(c.items())[:25]  # Counts similar words separately\n",
      "for item in sorted(c.items())[:25]:\n",
      "    print item[0], item[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[(u'A', 1),\n",
        " (u'Action', 1),\n",
        " (u'Although', 2),\n",
        " (u'American', 1),\n",
        " (u'An', 1),\n",
        " (u'April', 2),\n",
        " (u'Areas', 1),\n",
        " (u'As', 1),\n",
        " (u'Association', 1),\n",
        " (u'Big', 2),\n",
        " (u'Board', 1),\n",
        " (u'C.', 1),\n",
        " (u'C.F', 1),\n",
        " (u'CODATA', 1),\n",
        " (u'Carver', 1),\n",
        " (u'Century', 1),\n",
        " (u'Chandra', 1),\n",
        " (u'Classification', 1),\n",
        " (u'Cleveland', 2),\n",
        " (u'Collections', 1),\n",
        " (u'Columbia', 1),\n",
        " (u'Committee', 1),\n",
        " (u'Computer', 1),\n",
        " (u'Concise', 1),\n",
        " (u'Council', 1)]"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}