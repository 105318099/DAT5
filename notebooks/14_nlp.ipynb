{
 "metadata": {
  "name": "",
  "signature": "sha256:54bc3f860a29d74610705820116cf0e8971d57cdc3a605ccb0ceb6fd62a1cfb2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Natural Language Processing (NLP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is NLP?\n",
      "* Using computers to process (analyze, understand, generate) natural human languages\n",
      "\n",
      "Why NLP?\n",
      "* Most knowledge created by humans is unstructured text, so we need some way to make sense of it.\n",
      "* Enables quantitative analysis of text data at large scale.\n",
      "* Provides a repeatable, \"unbiased\" way to look at text."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Imports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy # Twitter API wrapper\n",
      "import nltk # Classic NLP package"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Variables that contains the user credentials to access Twitter API \n",
      "#access_token = \"2902716317-RrOXZgj6rnlketxHaYFimJwteiCa1Tutz59iDxb\"\n",
      "#access_token_secret = \"LmJ4D5pHKZKtLEHiwQGqPyFU2U8hSbGhUwY082RutBuwZ\"\n",
      "consumer_key = \"zBFpCeTFfHV8Tuog8DZF8lfb9\"\n",
      "consumer_secret = \"9NKNbyClhMaRZtKOPcDyIMef9eAKztOuHydGgsh54XJkxqEfo4\"\n",
      "\n",
      "# Create authorization for API\n",
      "auth = tweepy.auth.OAuthHandler(consumer_key, consumer_secret)\n",
      "#auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "# Initialize API object by passing it your credentials\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "# Use the api to search\n",
      "tweets = api.search(q=\"data science\", count=10, result_type=\"recent\")\n",
      "print tweets[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Status(contributors=None, truncated=False, text=u'Visualising data: both a science and an art http://t.co/kuF0SgYxyq', in_reply_to_status_id=None, id=594128161354420224L, favorite_count=0, _api=<tweepy.api.API object at 0x0000000014DFDD30>, author=User(follow_request_sent=None, profile_use_background_image=True, _json={u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 59061231, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/8fO5xujMn0', u'indices': [0, 22], u'expanded_url': u'http://www.goranspolicy.com', u'display_url': u'goranspolicy.com'}]}, u'description': {u'urls': []}}, u'followers_count': 422, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'59061231', u'profile_background_color': u'C0DEED', u'listed_count': 9, u'is_translation_enabled': False, u'utc_offset': None, u'statuses_count': 531, u'description': u'Promoting policy research and think tanks in Balkans and CE Europe. Co-directing the Open Society Initiative for Europe. Worries about Europe, loves jazz&coffee', u'friends_count': 216, u'location': u'Budapest, Hungary', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/59061231/1420628514', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'gbuldioski', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 98, u'name': u'Goran Buldioski', u'notifications': None, u'url': u'http://t.co/8fO5xujMn0', u'created_at': u'Wed Jul 22 06:57:28 +0000 2009', u'contributors_enabled': False, u'time_zone': None, u'protected': False, u'default_profile': True, u'is_translator': False}, time_zone=None, id=59061231, _api=<tweepy.api.API object at 0x0000000014DFDD30>, verified=False, profile_text_color=u'333333', profile_image_url_https=u'https://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', profile_sidebar_fill_color=u'DDEEF6', is_translator=False, geo_enabled=False, entities={u'url': {u'urls': [{u'url': u'http://t.co/8fO5xujMn0', u'indices': [0, 22], u'expanded_url': u'http://www.goranspolicy.com', u'display_url': u'goranspolicy.com'}]}, u'description': {u'urls': []}}, followers_count=422, protected=False, id_str=u'59061231', default_profile_image=False, listed_count=9, lang=u'en', utc_offset=None, statuses_count=531, description=u'Promoting policy research and think tanks in Balkans and CE Europe. Co-directing the Open Society Initiative for Europe. Worries about Europe, loves jazz&coffee', friends_count=216, profile_link_color=u'0084B4', profile_image_url=u'http://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', notifications=None, profile_background_image_url_https=u'https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_color=u'C0DEED', profile_banner_url=u'https://pbs.twimg.com/profile_banners/59061231/1420628514', profile_background_image_url=u'http://abs.twimg.com/images/themes/theme1/bg.png', name=u'Goran Buldioski', is_translation_enabled=False, profile_background_tile=False, favourites_count=98, screen_name=u'gbuldioski', url=u'http://t.co/8fO5xujMn0', created_at=datetime.datetime(2009, 7, 22, 6, 57, 28), contributors_enabled=False, location=u'Budapest, Hungary', profile_sidebar_border_color=u'C0DEED', default_profile=True, following=False), _json={u'contributors': None, u'truncated': False, u'text': u'Visualising data: both a science and an art http://t.co/kuF0SgYxyq', u'in_reply_to_status_id': None, u'id': 594128161354420224L, u'favorite_count': 0, u'source': u'<a href=\"http://www.facebook.com/twitter\" rel=\"nofollow\">Facebook</a>', u'retweeted': False, u'coordinates': None, u'entities': {u'symbols': [], u'user_mentions': [], u'hashtags': [], u'urls': [{u'url': u'http://t.co/kuF0SgYxyq', u'indices': [44, 66], u'expanded_url': u'http://fb.me/6zBsFQaqz', u'display_url': u'fb.me/6zBsFQaqz'}]}, u'in_reply_to_screen_name': None, u'in_reply_to_user_id': None, u'retweet_count': 0, u'id_str': u'594128161354420224', u'favorited': False, u'user': {u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 59061231, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/8fO5xujMn0', u'indices': [0, 22], u'expanded_url': u'http://www.goranspolicy.com', u'display_url': u'goranspolicy.com'}]}, u'description': {u'urls': []}}, u'followers_count': 422, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'59061231', u'profile_background_color': u'C0DEED', u'listed_count': 9, u'is_translation_enabled': False, u'utc_offset': None, u'statuses_count': 531, u'description': u'Promoting policy research and think tanks in Balkans and CE Europe. Co-directing the Open Society Initiative for Europe. Worries about Europe, loves jazz&coffee', u'friends_count': 216, u'location': u'Budapest, Hungary', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/59061231/1420628514', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'gbuldioski', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 98, u'name': u'Goran Buldioski', u'notifications': None, u'url': u'http://t.co/8fO5xujMn0', u'created_at': u'Wed Jul 22 06:57:28 +0000 2009', u'contributors_enabled': False, u'time_zone': None, u'protected': False, u'default_profile': True, u'is_translator': False}, u'geo': None, u'in_reply_to_user_id_str': None, u'possibly_sensitive': False, u'lang': u'en', u'created_at': u'Fri May 01 13:16:06 +0000 2015', u'in_reply_to_status_id_str': None, u'place': None, u'metadata': {u'iso_language_code': u'en', u'result_type': u'recent'}}, coordinates=None, entities={u'symbols': [], u'user_mentions': [], u'hashtags': [], u'urls': [{u'url': u'http://t.co/kuF0SgYxyq', u'indices': [44, 66], u'expanded_url': u'http://fb.me/6zBsFQaqz', u'display_url': u'fb.me/6zBsFQaqz'}]}, in_reply_to_screen_name=None, in_reply_to_user_id=None, retweet_count=0, id_str=u'594128161354420224', favorited=False, source_url=u'http://www.facebook.com/twitter', user=User(follow_request_sent=None, profile_use_background_image=True, _json={u'follow_request_sent': None, u'profile_use_background_image': True, u'default_profile_image': False, u'id': 59061231, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/8fO5xujMn0', u'indices': [0, 22], u'expanded_url': u'http://www.goranspolicy.com', u'display_url': u'goranspolicy.com'}]}, u'description': {u'urls': []}}, u'followers_count': 422, u'profile_sidebar_border_color': u'C0DEED', u'id_str': u'59061231', u'profile_background_color': u'C0DEED', u'listed_count': 9, u'is_translation_enabled': False, u'utc_offset': None, u'statuses_count': 531, u'description': u'Promoting policy research and think tanks in Balkans and CE Europe. Co-directing the Open Society Initiative for Europe. Worries about Europe, loves jazz&coffee', u'friends_count': 216, u'location': u'Budapest, Hungary', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', u'following': None, u'geo_enabled': False, u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/59061231/1420628514', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'gbuldioski', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 98, u'name': u'Goran Buldioski', u'notifications': None, u'url': u'http://t.co/8fO5xujMn0', u'created_at': u'Wed Jul 22 06:57:28 +0000 2009', u'contributors_enabled': False, u'time_zone': None, u'protected': False, u'default_profile': True, u'is_translator': False}, time_zone=None, id=59061231, _api=<tweepy.api.API object at 0x0000000014DFDD30>, verified=False, profile_text_color=u'333333', profile_image_url_https=u'https://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', profile_sidebar_fill_color=u'DDEEF6', is_translator=False, geo_enabled=False, entities={u'url': {u'urls': [{u'url': u'http://t.co/8fO5xujMn0', u'indices': [0, 22], u'expanded_url': u'http://www.goranspolicy.com', u'display_url': u'goranspolicy.com'}]}, u'description': {u'urls': []}}, followers_count=422, protected=False, id_str=u'59061231', default_profile_image=False, listed_count=9, lang=u'en', utc_offset=None, statuses_count=531, description=u'Promoting policy research and think tanks in Balkans and CE Europe. Co-directing the Open Society Initiative for Europe. Worries about Europe, loves jazz&coffee', friends_count=216, profile_link_color=u'0084B4', profile_image_url=u'http://pbs.twimg.com/profile_images/552782267215405057/Ie6hRlk-_normal.jpeg', notifications=None, profile_background_image_url_https=u'https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_color=u'C0DEED', profile_banner_url=u'https://pbs.twimg.com/profile_banners/59061231/1420628514', profile_background_image_url=u'http://abs.twimg.com/images/themes/theme1/bg.png', name=u'Goran Buldioski', is_translation_enabled=False, profile_background_tile=False, favourites_count=98, screen_name=u'gbuldioski', url=u'http://t.co/8fO5xujMn0', created_at=datetime.datetime(2009, 7, 22, 6, 57, 28), contributors_enabled=False, location=u'Budapest, Hungary', profile_sidebar_border_color=u'C0DEED', default_profile=True, following=False), geo=None, in_reply_to_user_id_str=None, possibly_sensitive=False, lang=u'en', created_at=datetime.datetime(2015, 5, 1, 13, 16, 6), in_reply_to_status_id_str=None, place=None, source=u'Facebook', retweeted=False, metadata={u'iso_language_code': u'en', u'result_type': u'recent'})\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets[0].text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "u'Open #DataScience Conference #ODSC to Bring Together the Best Minds: http://t.co/sJWCPgCYpm #BigData #Analytics\\n\\n&gt;&gt; http://t.co/PmkuDqS4Hy'"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some quick vocab:\n",
      "* **corpus** - collection of documents\n",
      "* **corpora** - plural form of corpus\n",
      "\n",
      "Let's build our corpus of tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets_text = []\n",
      "for tweet in tweepy.Cursor(api.search, q='kaggle', result_type='recent').items(1000):\n",
      "    tweets_text.append(tweet.text.encode('ascii','ignore'))\n",
      "print tweets_text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @benhamner: Machine learning gremlins: dozens of failure modes we've seen in real world machine learning at @kaggle https://t.co/Zjrea1y\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since I did not provide my credentials above, I saved all of the tweet text to a CSV file.  We can read it back into a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('../data/kaggle_tweets.csv','w') as f:\n",
      "    for tweet in tweets_text:\n",
      "        f.write('\"%s\"\\n' % tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('../data/kaggle_tweets.csv','r') as f:\n",
      "    tweets_text = [tweet.replace('\\n','').replace('\"','') for tweet in f.readlines()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets_text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "\"RT @benhamner: Machine learning gremlins: dozens of failure modes we've seen in real world machine learning at @kaggle https://t.co/Zjrea1y\""
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we need to do with our corpus of text is to break the documents into smaller units.  This is known as **tokenization**.  There are two natural ways to go about this:  breaking the documents apart into sentences or into words.  This gives more structure to the previously unstructured text.  This also allows us to more easily perform other tasks upon our corpus.\n",
      "\n",
      "**Note**:  Breaking documents and paragraphs into sentences and words is easier is some languages than others.  English has obvious (to us) breaks in the text for sentences and words.  However, this might not be the case for other languages.  In addition, there are nuances in the English language with hyphenated words and phrases that are independent clauses but might be part of a larger sentence.\n",
      "\n",
      "First let's try breaking our tweets down into sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize into sentences\n",
      "sentences = []\n",
      "for tweet in tweets_text:\n",
      "    for sent in nltk.sent_tokenize(tweet):\n",
      "        sentences.append(sent)\n",
      "sentences[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[\"RT @benhamner: Machine learning gremlins: dozens of failure modes we've seen in real world machine learning at @kaggle https://t.co/Zjrea1y\",\n",
        " 'RT @MKTJimmyxu: 10 R Packages to Win Kaggle Competitions by @DataRobot #gbm #glmnet http://t.co/lX6BhgRgH1 via @SlideShare',\n",
        " 'Top stories for Apr 19-25: Top LinkedIn Groups for Analytics, Big Data, Data Mining; 10 R Packages for a Kaggle Ch... http://t.co/f5Hkn2wkHB',\n",
        " 'RT @kaggle: scikit-learn #machinelearning video #4: Model training &amp; prediction with k-Nearest Neighbors http://t.co/JtR5945jLU http://t.co',\n",
        " 'RT @rohanrao: Doing my part to bring about the singularity.',\n",
        " '#MachineLearning #kaggle - https://t.co/icnG2mxPJb',\n",
        " 'RT @kaggle: scikit-learn #machinelearning video #4: Model training &amp; prediction with k-Nearest Neighbors http://t.co/JtR5945jLU http://t.co',\n",
        " 'RT @kaggle: scikit-learn #machinelearning video #4: Model training &amp; prediction with k-Nearest Neighbors http://t.co/JtR5945jLU http://t.co',\n",
        " 'RT @MKTJimmyxu: 10 R Packages to Win Kaggle Competitions by @DataRobot #gbm #glmnet http://t.co/lX6BhgRgH1 via @SlideShare',\n",
        " 'RT @benhamner: Yearly maps of West Nile Virus mosquito detections around Chicago https://t.co/sR5ZhO80ii http://t.co/zDq3c6OnvG']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's break our tweets into individual words, referred to as tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize into words\n",
      "tokens = []\n",
      "for tweet in tweets_text:\n",
      "    for word in nltk.word_tokenize(tweet):\n",
      "        tokens.append(word)\n",
      "tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['RT',\n",
        " '@',\n",
        " 'benhamner',\n",
        " ':',\n",
        " 'Machine',\n",
        " 'learning',\n",
        " 'gremlins',\n",
        " ':',\n",
        " 'dozens',\n",
        " 'of']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Only keep tokens that start with a letter (using regular expressions)\n",
      "import re\n",
      "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
      "clean_tokens[:20]# Tokenize into words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['RT',\n",
        " 'benhamner',\n",
        " 'Machine',\n",
        " 'learning',\n",
        " 'gremlins',\n",
        " 'dozens',\n",
        " 'of',\n",
        " 'failure',\n",
        " 'modes',\n",
        " 'we',\n",
        " 'seen',\n",
        " 'in',\n",
        " 'real',\n",
        " 'world',\n",
        " 'machine',\n",
        " 'learning',\n",
        " 'at',\n",
        " 'kaggle',\n",
        " 'https',\n",
        " 'RT']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now perform the \"hello world\" task of text analysis and get a list of the most popular words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count the tokens\n",
      "from collections import Counter\n",
      "c = Counter(clean_tokens)\n",
      "c.most_common(25) # Most frequent tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[('http', 857),\n",
        " ('kaggle', 461),\n",
        " ('RT', 459),\n",
        " ('Kaggle', 392),\n",
        " ('https', 315),\n",
        " ('to', 285),\n",
        " ('for', 227),\n",
        " ('the', 196),\n",
        " ('a', 177),\n",
        " ('Top', 135),\n",
        " ('BigData', 127),\n",
        " ('and', 120),\n",
        " ('on', 118),\n",
        " ('R', 117),\n",
        " ('with', 110),\n",
        " ('up', 109),\n",
        " ('in', 107),\n",
        " ('MachineLearning', 104),\n",
        " ('DataScience', 104),\n",
        " ('I', 103),\n",
        " ('video', 103),\n",
        " ('of', 101),\n",
        " ('machinelearning', 89),\n",
        " ('new', 82),\n",
        " ('Packages', 81)]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What do you notice about this list of words?  Are there any duplicated words?  What should we do about that?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming and Lemmatizing (Normalizing)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stemming** reduces a word to its base (stem) form.  It often makes sense to treat multipe word forms the same way.  \n",
      "\n",
      "Stemming uses a \"simple\" rule-based approach that runs very quickly.  The output isn't always the best for irregular words.  Stemmed words are not usually shown to users but rather used for analysis/indexing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize stemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "stemmer = SnowballStemmer('english')\n",
      "\n",
      "# Some exmaples\n",
      "print 'charge:', stemmer.stem('charge')\n",
      "print 'charging:', stemmer.stem('charging')\n",
      "print 'charged:', stemmer.stem('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "charge: charg\n",
        "charging: charg\n",
        "charged: charg\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's stem all of our tokens and recompute the count of most popular tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stem the tokens\n",
      "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
      "\n",
      "# Count the stemmed tokens\n",
      "c = Counter(stemmed_tokens)\n",
      "c.most_common(25)       # all lowercase"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[(u'http', 857),\n",
        " (u'kaggl', 855),\n",
        " ('rt', 459),\n",
        " (u'https', 315),\n",
        " ('to', 285),\n",
        " (u'for', 228),\n",
        " (u'the', 210),\n",
        " ('a', 196),\n",
        " (u'machinelearn', 193),\n",
        " (u'competit', 165),\n",
        " (u'bigdata', 139),\n",
        " (u'top', 138),\n",
        " (u'datasci', 134),\n",
        " (u'and', 121),\n",
        " ('on', 120),\n",
        " ('r', 118),\n",
        " (u'new', 113),\n",
        " (u'with', 112),\n",
        " (u'data', 110),\n",
        " ('in', 110),\n",
        " ('up', 110),\n",
        " ('i', 106),\n",
        " (u'video', 105),\n",
        " ('of', 101),\n",
        " (u'predict', 100)]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, some of these are still a bit uninterpretable for humans.  That's where lemmatizing comes in.\n",
      "\n",
      "**Lemmatization**, or normalization, dervies the canonical form (i.e. lemma) of a word.  This can be better than stemming in some cases, because it reduces words to a \"normal\" form.  This often uses a dictionary based approach and can be slower than stemming.  This is the tradeoff for \"better\" results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize lemmatizer\n",
      "lemmatizer = nltk.WordNetLemmatizer()\n",
      "\n",
      "# Compare stemmer to lemmatizer\n",
      "print 'dogs - stemmed:', stemmer.stem('dogs'), ', lemmatized:', lemmatizer.lemmatize('dogs')\n",
      "\n",
      "print 'wolves - stemmed:', stemmer.stem('wolves'), ', lemmatized:', lemmatizer.lemmatize('wolves')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dogs - stemmed: dog , lemmatized: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dog\n",
        "wolves - stemmed: wolv , lemmatized: wolf\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's lemmatize our Twitter dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lemmatize the tokens\n",
      "lemmatized_tokens = [lemmatizer.lemmatize(t).lower() for t in clean_tokens] # I lowercased things too.\n",
      "\n",
      "# Count the stemmed tokens\n",
      "c = Counter(lemmatized_tokens)\n",
      "c.most_common(25)       # all lowercase"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[(u'http', 1172),\n",
        " ('kaggle', 853),\n",
        " ('rt', 459),\n",
        " ('to', 285),\n",
        " ('for', 228),\n",
        " ('the', 210),\n",
        " ('a', 197),\n",
        " ('machinelearning', 193),\n",
        " ('competition', 159),\n",
        " ('bigdata', 139),\n",
        " ('top', 138),\n",
        " ('datascience', 134),\n",
        " ('and', 121),\n",
        " ('on', 120),\n",
        " ('r', 118),\n",
        " ('new', 113),\n",
        " ('with', 112),\n",
        " ('data', 110),\n",
        " ('in', 110),\n",
        " ('up', 110),\n",
        " ('i', 106),\n",
        " ('video', 105),\n",
        " ('of', 101),\n",
        " ('facebook', 82),\n",
        " ('is', 82)]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lemmatizing didn't do much here since msot of the popular words don't have significantly different normal forms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# One more example\n",
      "print 'is - stemmed:', stemmer.stem('is'), ', lemmatized:', lemmatizer.lemmatize('is')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "is - stemmed: is , lemmatized: is\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is not what I learned in grammar school.  Why isn't the result \"be\"? \n",
      "\n",
      "The lemmatizer assumes everything is a noun unless explicitly told otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer.lemmatize('is',pos='v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "u'be'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stopword Removal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stopwords** are common words that will most likely appear in any text.  They are \"useless\" words that don't contain much information.  For the purpose of word counts and other word frequencies, they are not particularly useful.  \n",
      "\n",
      "Let's remove the stopwords from our tweets and look at the most popular words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# View the list of stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "print stopwords[0:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they']\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stem the stopwords\n",
      "stemmed_stops = [stemmer.stem(t) for t in stopwords]\n",
      "\n",
      "# Remove stopwords from stemmed tokens\n",
      "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in stemmed_stops]\n",
      "c = Counter(stemmed_tokens_no_stop)\n",
      "most_common_stemmed = c.most_common(25)\n",
      "\n",
      "# Remove stopwords from cleaned tokens\n",
      "clean_tokens_no_stop = [t.lower() for t in clean_tokens if t.lower() not in stopwords]\n",
      "c = Counter(clean_tokens_no_stop)\n",
      "most_common_not_stemmed = c.most_common(25)\n",
      "\n",
      "# Compare the most common results for stemmed words and non stemmed words\n",
      "for i in range(25):\n",
      "    text_list = most_common_stemmed[i][0] + '  ' + str(most_common_stemmed[i][1]) + ' '*25\n",
      "    text_list = text_list[0:30]\n",
      "    text_list += most_common_not_stemmed[i][0] + '  ' + str(most_common_not_stemmed[i][1])\n",
      "    print text_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http  857                     http  857\n",
        "kaggl  856                    kaggle  853\n",
        "rt  459                       rt  459\n",
        "https  315                    https  315\n",
        "machinelearn  193             machinelearning  193\n",
        "competit  165                 bigdata  139\n",
        "bigdata  139                  top  138\n",
        "top  138                      competition  138\n",
        "datasci  134                  datascience  134\n",
        "r  118                        r  118\n",
        "new  113                      new  113\n",
        "data  110                     data  110\n",
        "video  105                    video  103\n",
        "predict  100                  facebook  82\n",
        "facebook  82                  packages  82\n",
        "packag  82                    intro  78\n",
        "intro  78                     via  76\n",
        "via  76                       series  72\n",
        "spot  74                      spots  72\n",
        "analyt  74                    analytics  68\n",
        "seri  72                      champion  66\n",
        "champion  66                  stories  61\n",
        "stori  63                     kdnuggets  60\n",
        "kdnugget  60                  scikit-learn  59\n",
        "scikit-learn  59              prediction  53\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These results are a bit more interesting.  You can see the most popular words that occur in the tweets about \"kaggle\"."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Named Entity Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Named Entity Recognition (NER)** is the automatic extraction of names, places, organizations, etc.  This can help you identify \"important\" words.  NER classifiers can work in different ways, but the most interesting and relevant ones use some sort of supervised machine learning technique.  There is some sort of tagged dataset that has a model/algorithm fit to it.  With what we've learned in class so far, you could build your own NER classifier!  However, it's often better to use existing classifiers.  \n",
      "\n",
      "First, let's build a NER extraction function that takes in a sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_entities(text):\n",
      "    entities = []\n",
      "    # tokenize into sentences\n",
      "    for sentence in nltk.sent_tokenize(text):\n",
      "        # tokenize sentences into words\n",
      "        # add part-of-speech tags\n",
      "        # use NLTK's NER classifier\n",
      "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
      "        # parse the results\n",
      "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
      "    return entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at all of the words in this dataset and see which named entities are identified.\n",
      "for entity in extract_entities('Kevin and Brandon are instructors for General Assembly in Washington, D.C.'):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[PERSON] Kevin\n",
        "[PERSON] Brandon\n",
        "[ORGANIZATION] General Assembly\n",
        "[GPE] Washington\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems to work pretty well!  But how resilient is it?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for entity in extract_entities('kevin and BRANDON are instructors for @GA_DC, D.C.'):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ORGANIZATION] BRANDON\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The accuracy decreased dramatically!  There are companies who are working to solve this problem, but as you get into more unstructured, \"wild\" data (like social media), this gets harder to do. \n",
      "\n",
      "We could run this on our entire dataset, but it would take a while, so I'll provide the code, but not actually run it.\n",
      "\n",
      "``` {python}\n",
      "named_entities = []\n",
      "for tweet in tweets_text:\n",
      "    temp_entities = extract_entities(tweet)\n",
      "    for temp_entity in temp_entities:\n",
      "        named_entities.append((temp_entity.label(), temp_entity.leaves()[0][0]))\n",
      "```\n",
      "\n",
      "Let's at least run it on one tweet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tweets_text[1]\n",
      "for entity in extract_entities(tweets_text[1]):\n",
      "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @MKTJimmyxu: 10 R Packages to Win Kaggle Competitions by @DataRobot #gbm #glmnet http://t.co/lX6BhgRgH1 via @SlideShare\n",
        "[ORGANIZATION] MKTJimmyxu\n",
        "[PERSON] Win Kaggle Competitions\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Topic Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic modeling allows us to discover \"topics\" in our dataset.  You can cluster words or documents.\n",
      "\n",
      "**Latent Dirichlet Allocation (LDA)** is a topic modeling method that allows us to discover clusters of words that appear together frequently."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lda # Latent Dirichlet Allocation\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Instantiate a count vectorizer with two additional parameters\n",
      "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
      "sentences_train = vect.fit_transform(np.array(tweets_text))\n",
      "\n",
      "# Instantiate an LDA model\n",
      "model = lda.LDA(n_topics=10, n_iter=500)\n",
      "model.fit(sentences_train) # Fit the model \n",
      "n_top_words = 10\n",
      "topic_word = model.topic_word_\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:lda:all zero row in document-term matrix found\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 1: kaggle, https, kaggle https, http, spots, competition, moved, kaggle http, rt\n",
        "Topic 2: http, kaggle, rt, 10, 10 packages kaggle, packages kaggle, 10 packages, packages kaggle champion, bigdata\n",
        "Topic 3: http, kaggle, facebook, learn, prediction, competition, scikit learn, rt, scikit\n",
        "Topic 4: kaggle, https, kaggle https, data, http, spots, rt, machinelearning, spots kaggle\n",
        "Topic 5: http, rt, datascience, kaggle, machinelearning, intro, datascience bigdata, intro machinelearning, bigdata\n",
        "Topic 6: http, kaggle, rt, based, classification, methods, tree, comparing, comparing tree\n",
        "Topic 7: http, kaggle, rt, top10, machinelearning, competitions, python, python machinelearning, 2nd\n",
        "Topic 8: http, kaggle, https, rt, virus, west nile virus, nile virus, west, nile\n",
        "Topic 9: https, kaggle, rt, kaggle https, data, places, predict, west nile, nile"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Topic 10: http, kaggle, rt, kaggle http, new, machinelearning, learn, datascience, series\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Get Data Science Wiki page\n",
      "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
      "b = BeautifulSoup(r.text)\n",
      "paragraphs = b.find(\"body\").findAll(\"p\")\n",
      "text = \"\"\n",
      "for paragraph in paragraphs:\n",
      "    text += paragraph.text + \" \"\n",
      "\n",
      "# Data Science corpus\n",
      "text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "u'In general terms, Data Science is the extraction of knowledge from data.[1][2] It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information theory and information technology, including signal processing, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization, predictive analytics, uncertainty modeling, data warehousing, data compression and high'"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into sentences\n",
      "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
      "sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "u'In general terms, Data Science is the extraction of knowledge from data.'"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instantiate a count vectorizer with two additional parameters\n",
      "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
      "sentences_train = vect.fit_transform(sentences)\n",
      "\n",
      "# Instantiate an LDA model\n",
      "model = lda.LDA(n_topics=10, n_iter=500)\n",
      "model.fit(sentences_train) # Fit the model \n",
      "n_top_words = 10\n",
      "topic_word = model.topic_word_\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 1: research, sciences, medical, long, digital data, safety efficacy novel, safety efficacy, safety, compounds\n",
        "Topic 2: learning, machine, machine learning, processing, business, statistics, intelligence, statistics machine, signal\n",
        "Topic 3: science, data science, data, term, used, term data, statistics, term data science, methods\n",
        "Topic 4: areas, models, technical, statistics, computing, technical areas, discipline, cleveland, theory\n",
        "Topic 5: data, scientists, data scientists, marketing, statistics machine learning, present, expert, required, large\n",
        "Topic 6: security, fraud, insights, security fraud, security data science, security data, data science data, science data, information security\n",
        "Topic 7: journal, domains, international, classification, conference, finance, university, april, social\n",
        "Topic 8: data, clinical, information, analysis, computer, applications, clinical data, published, statistical\n",
        "Topic 9: scientist, data scientist, statistical, statistician, knowledge, subject, mahalanobis, branch, indian\n",
        "Topic 10: data, science, data science, field, big data, big, techniques, computing data, structured\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sentiment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentiment allows us to convert a limited range of emotion into a number.  It gives us an idea of how \"positive\", \"negative\", or \"neutral\" a piece of text is."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remember our sentiment function from the API's class?\n",
      "def get_sentiment(text):\n",
      "    # Endpoint\n",
      "    url = 'http://www.datasciencetoolkit.org/text2sentiment/'\n",
      "    \n",
      "    # Specify header    \n",
      "    header = {'content-type': 'application/json'}\n",
      "    \n",
      "    # Next we specify the body (the information we want the API to work on)\n",
      "    body = text\n",
      "    \n",
      "    # Now we make the request\n",
      "    response = requests.post(url, data=body, headers=header)\n",
      "    # Notice that this is a POST request\n",
      "    \n",
      "    # Get the JSON from the response\n",
      "    sentiment = response.json()['score']\n",
      "    return sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# As a reminder of what this does\n",
      "print get_sentiment('I love pizza')\n",
      "print get_sentiment('I hate pizza')\n",
      "print get_sentiment('I feel nothing about pizza')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.0\n",
        "-3.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this idea of sentiment in mind, let's see how positive, negative, and neutral people are about our Kaggle tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's loop through our tweets and calculate sentiment\n",
      "from textblob import TextBlob\n",
      "\n",
      "sentiments = [TextBlob(tweet).sentiment.polarity for tweet in tweets_text]\n",
      "print tweets_text[0], sentiments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @benhamner: Machine learning gremlins: dozens of failure modes we've seen in real world machine learning at @kaggle https://t.co/Zjrea1y -0.0583333333333\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_sentiment = np.sum(sentiments)/len(sentiments)\n",
      "print avg_sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0896016606992\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "sns.distplot(sentiments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x2614be10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEDCAYAAAAhsS8XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbBD/H7JF4Htm3pi0AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2614b898>"
       ]
      }
     ],
     "prompt_number": 97
    }
   ],
   "metadata": {}
  }
 ]
}